---
title: "Homework 3"
author: "Iris Foxfoot"
date: "2/20/2022"
output: 
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) 
library(ISLR) 
library(glmnet) 
library(tree) 
library(maptree) 
library(randomForest) 
library(gbm) 
library(ROCR)
```

### Predicting carseats sales using regularized regression methods

```{r}
#read in data, split into testing and training

set.seed(123)
dat <- model.matrix(Sales~., Carseats) 
train = sample(nrow(dat), 30)
x.train = dat[train, ]
y.train = Carseats[train, ]$Sales
# The rest as test data
x.test = dat[-train, ]
y.test = Carseats[-train, ]$Sales
```

#### A. Ridge Regression

(2 pts) Fit a ridge regression model to the training set to predict Sales using all other variables as predictors. Use the built-in cross-validation in cv.glmnet to choose the optimal value of tuning parameter $\lambda$ from the following list of $\lambda$ values using a 5-fold CV. (2 pts) Report the ridge coefficient estimates corresponding to the selected value of $\lambda$.

```{r}
#list of lambdas
lambda.list.ridge = 1000 * exp(seq(0, log(1e-5), length = 100))

#writing the model
ridge_mod = cv.glmnet(x.train, y.train, alpha = 0, lambda = lambda.list.ridge, folds = 5)

#get best lambda value
bestlam = ridge_mod$lambda.min

#show best lambda value
bestlam

#show coefficients using lambda chosen by CV
predict(ridge_mod, type = "coefficients", s = bestlam)[1:13,] 
```


#### B. Ridge Regression MSE

(2 pts) What is the training MSE for the model corresponding to the optimal value of $\lambda$ selected by the cross-validation above? (2 pts) What is the test MSE for that same model? (1 pts) Comment on your findings.

```{r}
#show training MSE
ridge.pred.train=predict(ridge_mod, s=bestlam ,newx=x.train)
mean((ridge.pred.train-y.train)^2)

#show test MSE
ridge.pred.test=predict(ridge_mod,s=bestlam ,newx=x.test)
mean((ridge.pred.test-y.test)^2)
```

The training MSE is lower than the test MSE, indicating that the model may have overfit the data a little bit.

#### C. Lasso Model

(2 pts) Fit a lasso model to the training set to predict Sales using all other variables as predictors. Use the built-in cross-validation in cv.glmnet to choose the optimal value of tuning parameter $\lambda$ from the following list of $\lambda$ values using a 10-fold CV. (2 pts) Report the lasso coefficient estimates corresponding to the selected value of $\lambda$. (2 pts) Are there any coefficients set to zero in the model selected by cross-validation? Comment on your findings.

```{r}
#getting list of lambdas
lambda.list.lasso = 2 * exp(seq(0, log(1e-4), length = 100))

#building model (alpha set to zero for lasso)
lasso_mod = cv.glmnet(x.train, y.train, alpha = 1, lambda = lambda.list.lasso, folds = 10)

#get best lambda value
bestlam_l = lasso_mod$lambda.min

#show best lambda value
bestlam_l

#show coefficients using lambda chosen by CV
predict(lasso_mod, type = "coefficients", s = bestlam_l)[1:13,] 
```


#### D. Lasso Model MSE

(2 pts) What is the training MSE for the lasso model corresponding to the optimal value of $\lambda$ selected by cross-validation? (2 pts) What is the test MSE for that same model? (1 pts) 

# Comment on your findings.

```{r}
#show training MSE
lasso.pred.train=predict(lasso_mod, s=bestlam_l ,newx=x.train)
mean((lasso.pred.train-y.train)^2)

#show test MSE
lasso.pred.test=predict(lasso_mod,s=bestlam_l ,newx=x.test)
mean((lasso.pred.test-y.test)^2)
```

Again the training error is smaller than the test error, indicating some possible overfitting. 

#### E. Compare Ridge and Lasso

(2 pts) Comment on the comparison between ridge and lasso estimates in this application.

# Comment something

### Analyzing Drug Use

```{r}
#read in the data
drug <- read_csv('drug.csv',
                 col_names=c("ID", "Age", "Gender", "Education", "Country", "Ethnicity", "Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive", "SS", "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis", "Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh", "LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA"))
                             
```
#### A. Create new recent_cannabis_use col

(2 pts) Define a new factor response variable recent_cannabis_use which is “Yes” if a person has used cannabis within a year, and “No” otherwise. This can be done by checking if the Cannabis variable is greater than or equal to CL3. Hint: use mutate with the ifelse command. When creating the new factor set levels argument to levels=c("No", "Yes") (in that order).

```{r}
#creating col for recently used cannabis
drug <- drug %>% 
  mutate(recent_cannabis_use = as.factor(
    case_when(Cannabis %in% c("CL3", "CL4", "CL5", "CL6") ~ "Yes", T ~ "No")))

#checking levels
levels(drug$recent_cannabis_use)
#they look good
```

#### B. Subset the data

(b). (2 pts) We will only consider a subset of all predictors in subsequent tasks. To do so, we will create a new dataset that includes a subset of the original predictors. In particular, we will focus on all variables between age and SS (inclusively) as well as the new factor recent_cannabis_use you obtained in part (a).

```{r}
#creating a subset
drug_subset <- drug %>% 
  select(Age:SS, recent_cannabis_use)
```

#### C. Split into test and training

(2 pts) Split the dataset you obtained in part (b) into a training data set and a test data set. The training data should include 1100 randomly sampled observation and the test data should include the remaining observations. You will need the training and the test data for subsequent analysis.

```{r}
#get 1100 rows
drug_train_list = sample(nrow(drug_subset), 1100)

#get data for those rows
drug_train = drug_subset[drug_train_list, ]

#now everything else is test data
drug_test = drug_subset[-drug_train_list, ]
```

#### D. Logistic regression

(4 pts) As a benchmark method, fit a logistic regression to predict recent_cannabis_use using all other predictors in the training data you obtained in (c). Display the results by calling the summary function on the logistic regression object.

```{r}
#fit a logistic model
logistic <- glm(recent_cannabis_use ~ ., 
                data = drug_train,
                family = "binomial")

#show summary of model
summary(logistic)
```

#### E. Single tree

(e). (2 pts) Construct a single decision tree to predict recent_cannabis_use using all other predictors in the training data.

```{r}
single_tree <- tree(recent_cannabis_use ~ ., 
                data = drug_train)

summary(single_tree)
```

#### F. Cross validated single tree

(2 pts) Use 5-fold cross-validation to select the best size of a tree which minimizes the cross-validation estimate of the test error rate. Use the function cv.tree, and set the argument FUN=prune.misclass. If multiple trees have the same minimum cross validated error rate, set best_size to the smallest tree size with that minimum rate. (2 pts) Report the best size you obtained.

```{r}

```

